# -*- coding: utf-8 -*-
"""othello_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hq47yqh1_dicAaXftCJbl-2y9jy6Ap2o
"""

import numpy as np
import random
import matplotlib.pyplot as plt
from copy import deepcopy
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from tqdm.notebook import tqdm

#  1:black, -1:white

class Othello() :
    def __init__(self, board="new") :
      if board == "new" :
        self.board = np.zeros((8, 8))
        self.board[3, 3], self.board[4, 4] = (1, 1)
        self.board[3, 4], self.board[4, 3] = (-1, -1)
      else :
        self.board = board
      self.side = 1 # denotes which player is going to make a move now
      self.players = {1:"black", -1:"white"}
      self.passed = False
      self.update_available()
      self.game_over = False
      self.winner = 0
      self.round = 1
      self.last = deepcopy(self.board)
      self.log = ""

    def show(self) :
      """
      alphs = ["A", "B", "C", "D", "E", "F", "G", "H"]
      ids = [i for i in range(8)]
      plt.matshow(self.board, cmap=plt.get_cmap('binary'), alpha=1)
      # plt.grid(c="green", axis="x", which="minor", alpha=0.3, linewidth=1)
      plt.xticks(ids, np.array(ids)+1)
      plt.yticks(ids, alphs)
      plt.savefig("temp.png")
      # plt.show()
      plt.close()
      """
      pass
      
    def search(self, arr) :
        flipped = 0
        arr = list(arr)
        pos = arr.index(9)
        left = arr[:pos]
        right = arr[pos+1:]

        pivot = len(left)-1
        while pivot >= 0 :
          if left[pivot] == self.side*-1 :
            pivot -= 1
          elif left[pivot] == self.side :
            left[pivot:] = [self.side]*(len(left)-pivot)
            flipped += len(left) - pivot - 1
            break
          else :
            break
        pivot = 0  
        while len(right) > pivot :
          if right[pivot] == self.side*-1 :
            pivot += 1
          elif right[pivot] == self.side :
            right[0:pivot] = [self.side]*pivot
            flipped += pivot
            break
          else :
            break

        arr_new = left + [self.side] + right
        return flipped, arr_new
    
    def attempt(self, pos) : # check if the input is available
      pos = pos.lower()
      pos = (ord(pos[0])-97, int(pos[1])-1)
      pseudo_board = deepcopy(self.board)
      if pseudo_board[pos[0], pos[1]] != 0 : return 0, pseudo_board
      pseudo_board[pos[0], pos[1]] = 9
      row = pseudo_board[pos[0],:]
      col = pseudo_board[:,pos[1]]
      x1 = []
      x2 = []
      for i in range(8) :
        p1 = pos[1] - pos[0]
        if 0 <= i+p1 < 8 :
          x1.append(pseudo_board[i, i+p1])
        p2 = pos[1] + pos[0]
        if 0 <= p2-i < 8 :
          x2.append(pseudo_board[i, p2-i])
      a1, row = self.search(row)
      a2, col = self.search(col)
      a3, x1 = self.search(x1)
      a4, x2 = self.search(x2)
      pseudo_board[pos[0],:] = row
      pseudo_board[:,pos[1]] = col

      s1 = "?"
      p1 = pos[1] - pos[0]
      s2 = "?"
      p2 = pos[1] + pos[0]
      for i in range(8) :
        if 0 <= i+p1 < 8 :
          if s1 == "?" : s1 = i
          pseudo_board[i, i+p1] = x1[i-s1]
        if 0 <= p2-i < 8 :
          if s2 == "?" : s2 = i
          pseudo_board[i, p2-i] = x2[i-s2]

      return (a1 + a2 + a3 + a4), pseudo_board

    def update_available(self) :
      self.available = np.zeros((8, 8))
      for j, x in enumerate("12345678") :
        for i, y in enumerate("ABCDEFGH") :
          result = self.attempt(y+x)
          self.available[i, j] = result[0]

    def move(self, pos) :
      if type(pos) == int :
        pos = chr(pos//8+97) + str(pos%8+1)
        
      a, pseudo_board = self.attempt(pos)
      if a : 
        self.log += pos
        self.last = self.board
        self.board = pseudo_board
        #print(f"reward : {self.calc_reward()}")
        self.side *= -1 
        self.round += 1
        #print(f"Made a move at {pos} Successfully!")
        self.show()
        self.update_available()
        if (self.available == 0).all() :
          self.side *= -1
          self.update_available()
          if (self.available == 0).all() :
            #print("Both players have nowhere to move now, game over!")
            self.game_over = True
            black_p = np.count_nonzero(self.board == 1)
            white_p = np.count_nonzero(self.board == -1)
            if black_p > white_p : 
              #print("Black wins!")
              self.winner = 1
            elif black_p == white_p :
              #print("Tie!")
              self.winner = 0
            else :
              #print("White wins!")
              self.winner = -1

    def revert(self) :
      self.side *= -1
      self.board *= -1



class human() :
  def __init__(self) :
    pass
  def make_a_move(self, game) :
    return input()


class bot1() : # God does dot play dice, but this bot always does.
  def __init__(self) :
    pass
  def make_a_move(self, game) :
    candidates = np.where(game.available!=0)
    chosen_idx = random.randint(0, len(candidates[0])-1)
    return chr(candidates[0][chosen_idx]+97) + str(candidates[1][chosen_idx]+1) 


class bot2() : # A prototype bot which is only GREEDY
  def __init__(self) :
    pass
  def make_a_move(self, game) :
    candidates = np.where(game.available==np.amax(game.available))
    chosen_idx = random.randint(0, len(candidates[0])-1)
    return chr(candidates[0][chosen_idx]+97) + str(candidates[1][chosen_idx]+1)


class bot3() : # A moderate bot with somehow greedy strategy and knows the importance of getting control of the edges
  def __init__(self) :
    corner = np.zeros((8, 8))
    corner[0, 0], corner[0, 7], corner[7, 0], corner[7, 7] = (1, 1, 1, 1)
    self.corner = corner
    side = np.zeros((8, 8))
    side[0,:] = 1
    side[7,:] = 1
    side[:,0] = 1
    side[:,7] = 1
    side = side - corner
    self.side = side     
  def make_a_move(self, game) :
    if (self.corner*game.available != 0).any() :
      candidates = np.where(self.corner*game.available != 0)
    elif (self.side*game.available != 0).any() :
      candidates = np.where(self.side*game.available != 0)
    elif game.round < 20 :
      candidates = np.where(game.available==np.amin(game.available + (game.available==0)*20))
    else : 
      candidates = np.where(game.available==np.amax(game.available))
    chosen_idx = random.randint(0, len(candidates[0])-1)
    chosen = chr(candidates[0][chosen_idx]+97) + str(candidates[1][chosen_idx]+1)
    return chosen


class bot4() :
  def __init__(self, network) : 
    self.device = "cuda:0" if torch.cuda.is_available() else "cpu"
    self.decision_nn = network.to(self.device)
    self.reverted = False
  def make_a_move(self, game) :
    board = torch.from_numpy(game.board * game.side).float().to(self.device)
    with torch.no_grad() :
      self.decision_nn.eval()
      mat = self.decision_nn(board).cpu() * torch.from_numpy(np.sign(game.available))
      self.decision_nn.train()
    if (mat.detach().numpy() == 0).all() :
      return bot1().make_a_move(game)
    chosen = int(torch.argmax(mat))
    return chr(chosen//8+97) + str(chosen%8+1)

class Qnetwork(nn.Module) :
  def __init__(self) :
    super().__init__()
    self.fc = nn.Sequential(
        nn.Linear(64, 256),
        nn.ReLU(),
        nn.Linear(256, 1024),
        nn.ReLU(),
        nn.Linear(1024, 512),
        nn.ReLU(),
        nn.Linear(512, 256),
        nn.ReLU(),
        nn.Linear(256, 64),
        nn.ReLU()
    )
  def forward(self, x) :
    x = x.view(-1, 64)
    x = self.fc(x)
    return x.view(-1, 8, 8)

# an envivronment obj that simulates opponent moves, while given a Q-value network
class Environment() :
    def __init__(self, white_agent) :
      self.white_agent = white_agent
      self.reset()

    def reset(self) :
      self.game = Othello()
      if random.random() > 0.5 :
        self.game.board = -1*self.game.board
        self.game.update_available()
      self.last = self.game.board
      return self.game.board

    def step(self, action) :
      self.last = self.game.board
      self.game.move(action)
      while self.game.side == -1 and not self.game.game_over:
        # oppo_action = bot4(self.white_agent).make_a_move(self.game)
        oppo_action = bot3().make_a_move(self.game)
        self.game.move(oppo_action)
      return self.game.board, self.calc_reward(), self.game.game_over

    """
    win : +200
    get unflippable piece : +0.3*(60 - round)
    get side/corn : +3*(60 - round)
    total gain/loss of pieces : +2.5e-3 * x * round**2
    """
    def calc_reward(self) :
      rwp = 2.5e-3*(sum(sum(self.game.board))-sum(sum(self.last)))*(self.game.round**2)
      def calc_side(board) :
        return sum(board[0,:]) + sum(board[7,:]) + sum(board[:,0]) + sum(board[:,7])
      rws = 3*(calc_side(self.game.board)-calc_side(self.last))*(60-self.game.round)
      def calc_uf(board) :
        def search(arr) :
          arr = list(arr)
          if arr[0] == 0 :
            return np.array([0]*8)
          else : 
            domi = arr[0]
          for i in range(1, 8) :
            if arr[i] != domi :
              return np.array(arr[:i] + [0]*(8-i))
          return np.array(arr)
        a1 = board[0,:]
        a2 = board[7,:]
        a3 = board[:,0]
        a4 = board[:,7]
        a1p = sum(np.sign(search(a1) + search(a1[::-1])[-1]))
        a2p = sum(np.sign(search(a2) + search(a2[::-1])[-1]))
        a3p = sum(np.sign(search(a3) + search(a3[::-1])[-1]))
        a4p = sum(np.sign(search(a4) + search(a4[::-1])[-1]))
        return a1p + a2p + a3p + a4p
      rwu = 0.3*(calc_uf(self.game.board)-calc_uf(self.last))*(60-self.game.round)
      total_rw = (200*self.game.winner + rwp + rws + rwu) * self.game.side
      return total_rw

class ReplayBuffer():
    """Fixed-size buffer to store experience tuples."""

    def __init__(self, action_size=64, buffer_size=10000, batch_size=128, seed=48763):
        """Initialize a ReplayBuffer object.
        Params
        ======
            action_size (int): dimension of each action
            buffer_size (int): maximum size of buffer
            batch_size (int): size of each training batch
            seed (int): random seed
        """
        self.action_size = action_size
        self.memory = deque(maxlen=buffer_size)
        self.batch_size = batch_size
        self.experience = namedtuple("Experience", field_names=["state", "action", "reward", "next_state", "done"])
        self.seed = random.seed(seed)

    def add(self, state, action, reward, next_state, done):
        """Add a new experience to memory."""
        e = self.experience(state, action, reward, next_state, done)
        self.memory.append(e)

    def sample(self):
        """Randomly sample a batch of experiences from memory."""
        experiences = random.sample(self.memory, k=self.batch_size)

        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)
        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).to(device)
        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)
        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(
            device)
        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(
            device)

        return (states, actions, rewards, next_states, dones)

    def __len__(self):
        """Return the current size of internal memory."""
        return len(self.memory)

GAMMA = 0.98
TAU = 1e-3
UPDATE_EVERY = 4
BATCH_SIZE = 128
class PolicyGradientAgent():
    
    def __init__(self):
        self.qnetwork_local = Qnetwork().to(device)
        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=5e-4)
        self.qnetwork_target = Qnetwork().to(device)
        self.qnetwork_target.eval()
        self.memory = ReplayBuffer()

        self.t_step = 0

    def step(self, state, action, reward, next_state, done):
        # Save experience in replay memory
        action = 8*(ord(action[0]) - 97) + int(action[1]) - 1
        self.memory.add(state, action, reward, next_state, done)

        # Learn every UPDATE_EVERY time steps.
        self.t_step = (self.t_step + 1) % UPDATE_EVERY
        if self.t_step == 0:
            # If enough samples are available in memory, get random subset and learn
            if len(self.memory) > BATCH_SIZE:
                experiences = self.memory.sample()
                self.learn(experiences, GAMMA)

    def act(self, state, eps=0.):
        """Returns actions for given state as per current policy.
        Params
        ======
            state (array_like): current state
            eps (float): epsilon, for epsilon-greedy action selection
        """
        local_game = Othello(state)
        #print(local_game.available)
        #print(local_game.board)
        #print(local_game.game_over)
        # Epsilon-greedy action selection
        if random.random() > eps:
            return bot4(self.qnetwork_local).make_a_move(local_game)
        else:
            return bot1().make_a_move(local_game)
 
    def learn(self, experiences, gamma):
        """Update value parameters using given batch of experience tuples.
        Params
        ======
            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples
            gamma (float): discount factor
        """
        states, actions, rewards, next_states, dones = experiences

        # Get max predicted Q values (for next states) from target model
        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)
        # Compute Q targets for current states
        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))

        # Get expected Q values from local model
        Q_expected = self.qnetwork_local(states).flatten(1).gather(1, actions)

        # Compute loss
        loss = F.mse_loss(Q_expected, Q_targets)
        # Minimize the loss
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # ------------------- update target network ------------------- #
        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)

    def soft_update(self, local_model, target_model, tau):
        """Soft update model parameters.
        θ_target = τ*θ_local + (1 - τ)*θ_target
        Params
        ======
            local_model (PyTorch model): weights will be copied from
            target_model (PyTorch model): weights will be copied to
            tau (float): interpolation parameter
        """
        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):
            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)
        
    def sample(self, state):
        action_prob = self.network(torch.FloatTensor(state))
        action_dist = Categorical(action_prob)
        action = action_dist.sample()
        log_prob = action_dist.log_prob(action)
        return action.item(), log_prob

    def save(self, PATH): # You should not revise this
        Agent_Dict = {
            "network" : self.qnetwork_local.state_dict(),
            "optimizer" : self.optimizer.state_dict()
        }
        torch.save(Agent_Dict, PATH)

    def load(self, PATH): # You should not revise this
        checkpoint = torch.load(PATH)
        self.network.load_state_dict(checkpoint["network"])
        #如果要儲存過程或是中斷訓練後想繼續可以用喔 ^_^
        self.optimizer.load_state_dict(checkpoint["optimizer"])

from collections import namedtuple, deque
device = "cuda"
agent = PolicyGradientAgent()
env = Environment(agent.qnetwork_target)

def benchmark(agent, times=100) :
  players = { 1:bot4(agent.qnetwork_local), -1:bot3()}
  # players = { 1:bot4(Qnetwork()), -1:bot3()}
  wins = 0
  loses = 0
  simulations = times
  for _ in tqdm(range(simulations)) :
    testgame = Othello()
    while not testgame.game_over :
      testgame.move(players[testgame.side].make_a_move(testgame))
    wins += (testgame.winner==1)
    loses += (testgame.winner==-1)

  return wins/simulations, loses/simulations

# agent.network.train()  # 訓練前，先確保 network 處在 training 模式
EPISODE_PER_BATCH = 5  # 每蒐集 5 個 episodes 更新一次 agent
NUM_BATCH = 50000        # 總共更新 400 次
eps_start = 0
eps_end=0
eps_decay=0.995
best_result = 0


avg_total_rewards, avg_final_rewards = [], []

prg_bar = tqdm(range(NUM_BATCH))
eps = eps_start
for batch in prg_bar:

    log_probs, rewards, rewards_raw = [], [], []
    total_rewards, final_rewards = [], []
    env.white_agent = agent.qnetwork_target
    # 蒐集訓練資料
    
    for episode in range(EPISODE_PER_BATCH):
        
        state = env.reset()
        total_reward, total_step = 0, 0
        seq_rewards = []
        while True:

            action = agent.act(state, eps=eps) # at , log(at|st)
            next_state, reward, done = env.step(action)


            agent.step(state, action, reward, next_state, done)
            # seq_rewards.append(reward)
            state = next_state
            total_reward += reward
            total_step += 1
            # ! 重要 ！
            # 現在的reward 的implementation 為每個時刻的瞬時reward, 給定action_list : a1, a2, a3 ......
            #                               reward :     r1, r2 ,r3 ......
            # medium：將reward調整成accumulative decaying reward, 給定action_list : a1,                         a2,                           a3 ......
            #                               reward :     r1+0.99*r2+0.99^2*r3+......, r2+0.99*r3+0.99^2*r4+...... ,r3+0.99*r4+0.99^2*r5+ ......
            # boss : implement DQN
            if done or total_step > 1000:
                final_rewards.append(reward)
                total_rewards.append(total_reward)
                break
        eps = max(eps * eps_decay, eps_end)

    # rint(f"rewards looks like ", np.shape(rewards))  
    # print(f"log_probs looks like ", np.shape(log_probs))     
    # 紀錄訓練過程
    avg_total_reward = sum(total_rewards) / len(total_rewards)
    avg_final_reward = sum(final_rewards) / len(final_rewards)
    avg_total_rewards.append(avg_total_reward)
    avg_final_rewards.append(avg_final_reward)
    prg_bar.set_description(f"Total: {avg_total_reward: 4.1f}, Final: {avg_final_reward: 4.1f}")

    if batch%200 == 0 :
      agent.save("drive/MyDrive/colab_datas/othello/network_info")
      results = benchmark(agent)
      print(results)
      if results[0] > best_result :
        valid = benchmark(agent, 500)
        print(valid)
        if valid[0] > best_result :
          best_result = valid[0]
          agent.save("drive/MyDrive/colab_datas/othello/best_network_info")

benchmark(agent, 1000)